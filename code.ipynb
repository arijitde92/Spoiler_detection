{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Spoiler Detection from movie reviews\n",
    "This notebook implements a Multi layer perceptron model to be trained on movie reviews from the IMDB spoiler dataset (available [here](https://www.kaggle.com/datasets/rmisra/imdb-spoiler-dataset/data)).\n",
    "[spaCy](https://spacy.io/) is used to perform NLP tasks like tokenization, feature extraction and entity recognition.\n",
    "[Scikit-learn](https://scikit-learn.org/) is used to perform Term Frequence Inverse Document Frequency (TF-IDF) vectorization along with basic ML tasks like scaling, train test splitting, model creation and training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading and preprocessing\n",
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T10:11:07.652497400Z",
     "start_time": "2023-12-16T10:11:01.574973500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import concurrent\n",
    "import pickle\n",
    "from time import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from numpy import hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a function to find valid release dates and filter dataframe based on it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T10:11:07.666499800Z",
     "start_time": "2023-12-16T10:11:07.652497400Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_valid_release_dates(dataframe):\n",
    "    \"\"\"\n",
    "    Function that checks if the date value has 10 characters or not.\n",
    "    It returns a dataframe with only those rows where above criteria is true.\n",
    "    :param dataframe: dataframe that contains the movie reviews data\n",
    "                      along with their release date\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mask = (dataframe['release_date'].str.len() == 10)\n",
    "    return dataframe.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T10:11:16.192902400Z",
     "start_time": "2023-12-16T10:11:07.667501100Z"
    }
   },
   "outputs": [],
   "source": [
    "df_reviews = pd.read_json('data/IMDB_reviews.json', lines=True)\n",
    "df_movies = pd.read_json('data/IMDB_movie_details.json', lines=True)\n",
    "\n",
    "# Filter the dataframe to extract rows with valid movie release dates\n",
    "df_movies = filter_valid_release_dates(df_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Subset dataframe by taking only required columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T10:11:32.245567900Z",
     "start_time": "2023-12-16T10:11:16.194903300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   days_since_release                                    review_combined  \\\n",
      "0                4137  A classic piece of unforgettable film-making. ...   \n",
      "1                2154  Simply amazing. The best film of the 90's. The...   \n",
      "2                2485  The best story ever told on film I believe tha...   \n",
      "3                2879  Busy dying or busy living? **Yes, there are SP...   \n",
      "4                3506  Great story, wondrously told and acted At the ...   \n",
      "\n",
      "   is_spoiler  \n",
      "0        True  \n",
      "1        True  \n",
      "2        True  \n",
      "3        True  \n",
      "4        True  \n"
     ]
    }
   ],
   "source": [
    "# Convert date columns to datetime format\n",
    "df_reviews['review_date'] = pd.to_datetime(df_reviews['review_date'])\n",
    "df_movies['release_date'] = pd.to_datetime(df_movies['release_date'])\n",
    "\n",
    "# Merge the two dataframes on movie_id\n",
    "merged_df = pd.merge(df_reviews, df_movies, on='movie_id')\n",
    "\n",
    "# Calculate days since release for each review\n",
    "merged_df['days_since_release'] = (merged_df['review_date'] - merged_df['release_date']).dt.days\n",
    "\n",
    "merged_df['review_combined'] = merged_df['review_summary'].astype(str) + ' ' + merged_df['review_text'].astype(str)\n",
    "\n",
    "# Extract the combined text column and the label column\n",
    "selected_columns = ['days_since_release', 'review_combined', 'is_spoiler']\n",
    "subset_df = merged_df[selected_columns]\n",
    "\n",
    "print(subset_df.head())\n",
    "\n",
    "# Convert the subsetted data to CSV and store it\n",
    "subset_df.to_csv('data/review_with_labels_withDateDiff.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess data\n",
    "We take only the first 2000 samples due to limitation of computational resources. Feel free to adjust the samples taken based on your available computational resource."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T10:12:13.617963400Z",
     "start_time": "2023-12-16T10:11:32.256570500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "\n",
      "Step 1: Reading the CSV file using pandas DONE!\n",
      "Elapsed time: 00:00:07\n",
      "\n",
      "Step 2: Extracting the columns with labels and text DONE!\n",
      "Elapsed time: 00:00:00\n",
      "\n",
      "Step 3: Loading the spacy en_core_web_sm DONE!\n",
      "Elapsed time: 00:00:00\n",
      "\n",
      "Step 4: Apply text preprocessing to each text DONE!\n",
      "Elapsed time: 00:00:32\n",
      "\n",
      "Step 5: Dump the preprocessed data using Pickle DONE!\n",
      "Elapsed time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time()\n",
    "absolute_start_time = start_time\n",
    "\n",
    "df = pd.read_csv('data/review_with_labels_withDateDiff.csv')\n",
    "\n",
    "df = df[:2000]\n",
    "print(len(df))\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time() - start_time\n",
    "# Convert elapsed time to hours, minutes, and seconds\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "# Display the elapsed time\n",
    "print(\"\\nStep 1: Reading the CSV file using pandas DONE!\")\n",
    "print(f\"Elapsed time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# Step 2: Extract the columns with labels and text\n",
    "\n",
    "# Start the timer\n",
    "start_time = time()\n",
    "\n",
    "labels = df['is_spoiler'].tolist()\n",
    "texts = df['review_combined'].tolist()\n",
    "days_since_review = df['days_since_release'].tolist()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time() - start_time\n",
    "# Convert elapsed time to hours, minutes, and seconds\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "# Display the elapsed time\n",
    "print(\"\\nStep 2: Extracting the columns with labels and text DONE!\")\n",
    "print(f\"Elapsed time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# Step 3: Text preprocessing with spaCy\n",
    "\n",
    "# Start the timer\n",
    "start_time = time()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time() - start_time\n",
    "# Convert elapsed time to hours, minutes, and seconds\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "# Display the elapsed time\n",
    "print(\"\\nStep 3: Loading the spacy en_core_web_sm DONE!\")\n",
    "print(f\"Elapsed time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# Step 4: Apply text preprocessing to each text\n",
    "\n",
    "# Function to preprocess text and include tokens, POS tags, and named entities\n",
    "def preprocess(text, days):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "    entity_counts = doc.count_by(spacy.attrs.ENT_TYPE)\n",
    "\n",
    "    processed_text = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    tokens = ' '.join(processed_text)\n",
    "\n",
    "    # Extracting some common POS tags as features\n",
    "    pos_ner_counts = {\n",
    "        'NOUN_count': pos_counts.get(spacy.symbols.NOUN, 0),\n",
    "        'VERB_count': pos_counts.get(spacy.symbols.VERB, 0),\n",
    "        'ADJ_count': pos_counts.get(spacy.symbols.ADJ, 0),\n",
    "        'PERSON_count': entity_counts.get(spacy.symbols.PERSON, 0),\n",
    "        'ORG_count': entity_counts.get(spacy.symbols.ORG, 0),\n",
    "        'GPE_count': entity_counts.get(spacy.symbols.GPE, 0),\n",
    "        'DATE_count': entity_counts.get(spacy.symbols.DATE, 0)\n",
    "    }\n",
    "\n",
    "    features = [days]\n",
    "\n",
    "    for i in pos_ner_counts.keys():\n",
    "        features.append(pos_ner_counts[i])\n",
    "    features.append(tokens)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "start_time = time()\n",
    "\n",
    "# PARALLELIZATION CODE FOR SPACY\n",
    "\n",
    "# Specify the maximum number of worker threads\n",
    "max_workers = 2  # Set the number of desired worker threads\n",
    "processed_texts = []\n",
    "# Create a ThreadPoolExecutor with the specified number of worker threads\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit the text processing tasks to the executor\n",
    "    future_results = [executor.submit(preprocess, text, days) for text, days in zip(texts, days_since_review)]\n",
    "\n",
    "    # Retrieve the results as they become available\n",
    "\n",
    "    for future in concurrent.futures.as_completed(future_results):\n",
    "        processed_doc = future.result()\n",
    "        processed_texts.append(processed_doc)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time() - start_time\n",
    "# Convert elapsed time to hours, minutes, and seconds\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "# Display the elapsed time\n",
    "print(\"\\nStep 4: Apply text preprocessing to each text DONE!\")\n",
    "print(f\"Elapsed time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "######################################################################\n",
    "\n",
    "\n",
    "# Step 4.5: Dump the preprocessed data using Pickle\n",
    "\n",
    "# Save the processed_texts list to a file using Pickle\n",
    "with open('data/processed_texts_BEFORE_SCALING_full_with_pos_ner_counts_datediff.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_texts, f)\n",
    "\n",
    "# Create a DataFrame from the list of extracted features\n",
    "df = pd.DataFrame(processed_texts, columns=['Days', 'NOUN_count', 'VERB_count', 'ADJ_count', 'PERSON_count', 'ORG_count', 'GPE_count', 'DATE_count', 'tokens'])\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "normalized_counts = scaler.fit_transform(df[['Days','NOUN_count', 'VERB_count', 'ADJ_count', 'PERSON_count', 'ORG_count', 'GPE_count', 'DATE_count']])\n",
    "\n",
    "# Replace the original counts with the normalized counts\n",
    "df[['Days','NOUN_count', 'VERB_count', 'ADJ_count', 'PERSON_count', 'ORG_count', 'GPE_count', 'DATE_count']] = normalized_counts\n",
    "\n",
    "# Now, df contains the original columns with the normalized counts\n",
    "\n",
    "######################################################################\n",
    "# Step 5: Dump the preprocessed data using Pickle\n",
    "# Start the timer\n",
    "start_time = time()\n",
    "\n",
    "# Save the processed_texts list to a file using Pickle\n",
    "with open('data/processed_texts_full_with_pos_ner_counts_datediff.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "# Save the processed_texts list to a file using Pickle\n",
    "with open('data/labels_full_with_pos_ner_count_datediff.pkl', 'wb') as f:\n",
    "    pickle.dump(labels, f)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time() - start_time\n",
    "# Convert elapsed time to hours, minutes, and seconds\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "# Display the elapsed time\n",
    "print(\"\\nStep 5: Dump the preprocessed data using Pickle DONE!\")\n",
    "print(f\"Elapsed time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "\n",
    "# Save the labels list to a file using Pickle\n",
    "with open('data/labels_full_with_pos_ner_count_datediff.pkl', 'wb') as f:\n",
    "    pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read texts and labels using Pickle DONE!\n"
     ]
    }
   ],
   "source": [
    "with open('data/processed_texts_full_with_pos_ner_counts_datediff.pkl', 'rb') as f:\n",
    "    processed_texts = pickle.load(f)\n",
    "\n",
    "with open('data/labels_full_with_pos_ner_count_datediff.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "print(\"\\nRead texts and labels using Pickle DONE!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T10:12:13.663973700Z",
     "start_time": "2023-12-16T10:12:13.619963600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split the data into training and testing sets with 80:20 ratio"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Splitting data into training and testing sets DONE!\n",
      "Elapsed time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Splitting data into training and testing sets\n",
    "\n",
    "# Start the timer\n",
    "start_time = time()\n",
    "\n",
    "X = processed_texts\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time() - start_time\n",
    "# Convert elapsed time to hours, minutes, and seconds\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "# Display the elapsed time\n",
    "print(\"\\nStep 6: Splitting data into training and testing sets DONE!\")\n",
    "print(f\"Elapsed time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T10:12:13.683607300Z",
     "start_time": "2023-12-16T10:12:13.635967400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perform TF-IDF vectorization and save them into a pickle file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 7: Vectorization with TfidfVectorizer DONE!\n",
      "Elapsed time: 00:00:01\n",
      "\n",
      "Step 7.1: Dump X_train_vectorized, y_train, X_test_vectorized, y_test using Pickle DONE!\n",
      "Elapsed time: 00:00:01\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Vectorization with TfidfVectorizer\n",
    "\n",
    "# Start the timer\n",
    "start_time = time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.9, max_features=100000)\n",
    "X_train_tokens_vectorized = vectorizer.fit_transform(X_train['tokens'])\n",
    "X_test_tokens_vectorized = vectorizer.transform(X_test['tokens'])\n",
    "# print(\"Shape before expanding:\",X_train_tokens_vectorized.shape)\n",
    "\n",
    "# Converting to numpy array\n",
    "X_train_additional_features = X_train[['Days', 'NOUN_count', 'VERB_count', 'ADJ_count', 'PERSON_count', 'ORG_count', 'GPE_count', 'DATE_count']].to_numpy()\n",
    "X_test_additional_features = X_test[['Days', 'NOUN_count', 'VERB_count', 'ADJ_count', 'PERSON_count', 'ORG_count', 'GPE_count', 'DATE_count']].to_numpy()\n",
    "\n",
    "# print(\"Shape of vectorized tokens:\", X_train_tokens_vectorized.shape)\n",
    "# print(\"Shape of additional features:\", X_train_additional_features.shape)\n",
    "\n",
    "# Use hstack to combine the sparse TF-IDF matrices and the additional feature matrices\n",
    "X_train_combined = hstack((X_train_tokens_vectorized.toarray(), X_train_additional_features))\n",
    "X_test_combined = hstack((X_test_tokens_vectorized.toarray(), X_test_additional_features))\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time() - start_time\n",
    "# Convert elapsed time to hours, minutes, and seconds\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "# Display the elapsed time\n",
    "print(\"\\nStep 7: Vectorization with TfidfVectorizer DONE!\")\n",
    "print(f\"Elapsed time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "\n",
    "# Step 7.1: Dump X_train_vectorized, y_train, X_test_vectorized, y_test using Pickle\n",
    "\n",
    "# Start the timer\n",
    "start_time = time()\n",
    "\n",
    "with open('data/X_train_vectorized_full_ngram13_maxdf9_pos_ner_counts_strat_feat_100k.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_combined, f)\n",
    "\n",
    "with open('data/y_train_full_ngram13_maxdf9_pos_ner_counts_strat_feat_100k.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "\n",
    "with open('data/X_test_vectorized_full_ngram13_maxdf9_pos_ner_counts_strat_feat_100k.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test_combined, f)\n",
    "\n",
    "with open('data/y_test_full_ngram13_maxdf9_pos_ner_counts_strat_feat_100k.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time() - start_time\n",
    "# Convert elapsed time to hours, minutes, and seconds\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "# Display the elapsed time\n",
    "print(\"\\nStep 7.1: Dump X_train_vectorized, y_train, X_test_vectorized, y_test using Pickle DONE!\")\n",
    "print(f\"Elapsed time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T10:12:16.876692600Z",
     "start_time": "2023-12-16T10:12:13.677609400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the vectorized data and perform MLP classification.\n",
    "A review can be classified into either 'spoiler' or 'not spoiler'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T10:15:51.990290800Z",
     "start_time": "2023-12-16T10:12:16.883694600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOKENS+DATE+POS+NER, NGRAM: 1,3, MODEL: XGBOOST\n",
      "\n",
      "Step 7.9: Read X_train_vectorized, y_train, X_test_vectorized, y_test using Pickle DONE!\n",
      "Iteration 1, loss = 0.69324372\n",
      "Iteration 2, loss = 0.62046751\n",
      "Iteration 3, loss = 0.49461294\n",
      "Iteration 4, loss = 0.34296172\n",
      "Iteration 5, loss = 0.20572467\n",
      "Iteration 6, loss = 0.11481623\n",
      "Iteration 7, loss = 0.06674227\n",
      "Iteration 8, loss = 0.04659192\n",
      "Iteration 9, loss = 0.03579284\n",
      "Iteration 10, loss = 0.03274321\n",
      "Iteration 11, loss = 0.03121158\n",
      "Iteration 12, loss = 0.02828942\n",
      "Iteration 13, loss = 0.02511771\n",
      "Iteration 14, loss = 0.02248600\n",
      "Iteration 15, loss = 0.02087546\n",
      "Iteration 16, loss = 0.01888110\n",
      "Iteration 17, loss = 0.01800053\n",
      "Iteration 18, loss = 0.01690353\n",
      "Iteration 19, loss = 0.01579055\n",
      "Iteration 20, loss = 0.01477849\n",
      "Iteration 21, loss = 0.01337572\n",
      "Iteration 22, loss = 0.01458336\n",
      "Iteration 23, loss = 0.01395171\n",
      "Iteration 24, loss = 0.01285453\n",
      "Iteration 25, loss = 0.01226380\n",
      "Iteration 26, loss = 0.01170837\n",
      "Iteration 27, loss = 0.00991508\n",
      "Iteration 28, loss = 0.01069617\n",
      "Iteration 29, loss = 0.00994559\n",
      "Iteration 30, loss = 0.00953284\n",
      "Iteration 31, loss = 0.01027797\n",
      "Iteration 32, loss = 0.00973651\n",
      "Iteration 33, loss = 0.00896026\n",
      "Iteration 34, loss = 0.00970669\n",
      "Iteration 35, loss = 0.00824987\n",
      "Iteration 36, loss = 0.00978082\n",
      "Iteration 37, loss = 0.01013426\n",
      "Iteration 38, loss = 0.00784585\n",
      "Iteration 39, loss = 0.00766353\n",
      "Iteration 40, loss = 0.00807681\n",
      "Iteration 41, loss = 0.00796950\n",
      "Iteration 42, loss = 0.00660203\n",
      "Iteration 43, loss = 0.00758020\n",
      "Iteration 44, loss = 0.00654328\n",
      "Iteration 45, loss = 0.00838196\n",
      "Iteration 46, loss = 0.00619278\n",
      "Iteration 47, loss = 0.00690026\n",
      "Iteration 48, loss = 0.00627503\n",
      "Iteration 49, loss = 0.00601189\n",
      "Iteration 50, loss = 0.00884352\n",
      "Iteration 51, loss = 0.00740311\n",
      "Iteration 52, loss = 0.00670085\n",
      "Iteration 53, loss = 0.00668442\n",
      "Iteration 54, loss = 0.00772207\n",
      "Iteration 55, loss = 0.00626453\n",
      "Iteration 56, loss = 0.00674114\n",
      "Iteration 57, loss = 0.00591266\n",
      "Iteration 58, loss = 0.00549080\n",
      "Iteration 59, loss = 0.00457579\n",
      "Iteration 60, loss = 0.00551077\n",
      "Iteration 61, loss = 0.00481975\n",
      "Iteration 62, loss = 0.00496633\n",
      "Iteration 63, loss = 0.00422788\n",
      "Iteration 64, loss = 0.00452942\n",
      "Iteration 65, loss = 0.00446132\n",
      "Iteration 66, loss = 0.00477925\n",
      "Iteration 67, loss = 0.00442397\n",
      "Iteration 68, loss = 0.00464083\n",
      "Iteration 69, loss = 0.00546943\n",
      "Iteration 70, loss = 0.00441593\n",
      "Iteration 71, loss = 0.00481222\n",
      "Iteration 72, loss = 0.00419008\n",
      "Iteration 73, loss = 0.00581695\n",
      "Iteration 74, loss = 0.00528124\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "Step 8: Classification with SVC DONE!\n",
      "Elapsed time: 00:03:33\n",
      "\n",
      "Step 9: Evaluation DONE!\n",
      "Elapsed time: 00:00:00\n",
      "Accuracy: 0.5825\n",
      "Confusion Matrix:\n",
      "[[133  76]\n",
      " [ 91 100]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.59      0.64      0.61       209\n",
      "        True       0.57      0.52      0.54       191\n",
      "\n",
      "    accuracy                           0.58       400\n",
      "   macro avg       0.58      0.58      0.58       400\n",
      "weighted avg       0.58      0.58      0.58       400\n",
      "\n",
      "\n",
      "Step 10: Display the results DONE!\n",
      "Elapsed time: 00:00:00\n",
      "\n",
      "Step 11: Dump y_pred using Pickle DONE!\n",
      "calculating y_pred proba\n",
      "\n",
      "Step 11: Dump y_pred_proba using Pickle DONE!\n",
      "\n",
      "TOTAL ELAPSED TIME: 00:03:34\n"
     ]
    }
   ],
   "source": [
    "print(\"USING TOKENS+DATE+POS+NER, NGRAM: 1,3, MODEL: XGBOOST\")\n",
    "\n",
    "# Step 7.9: Read X_train_vectorized, y_train, X_test_vectorized, y_test using Pickle\n",
    "\n",
    "# Start the timer AGAIN!\n",
    "start_time = time()\n",
    "absolute_start_time = start_time\n",
    "\n",
    "with open('data/X_train_vectorized_full_ngram13_maxdf9_pos_ner_counts_strat_feat_100k.pkl', 'rb') as f:\n",
    "    X_train_vectorized = pickle.load(f)\n",
    "\n",
    "with open('data/y_train_full_ngram13_maxdf9_pos_ner_counts_strat_feat_100k.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "with open('data/X_test_vectorized_full_ngram13_maxdf9_pos_ner_counts_strat_feat_100k.pkl', 'rb') as f:\n",
    "    X_test_vectorized = pickle.load(f)\n",
    "\n",
    "with open('data/y_test_full_ngram13_maxdf9_pos_ner_counts_strat_feat_100k.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "print(\"\\nStep 7.9: Read X_train_vectorized, y_train, X_test_vectorized, y_test using Pickle DONE!\")\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# Start the timer\n",
    "start_time = time()\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100,100), activation='relu', solver='adam', random_state=42, verbose=1)\n",
    "\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time() - start_time\n",
    "# Convert elapsed time to hours, minutes, and seconds\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "# Display the elapsed time\n",
    "print(\"\\nStep 8: Classification with SVC DONE!\")\n",
    "print(f\"Elapsed time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "\n",
    "######################################################################\n",
    "# Step 9: Evaluation\n",
    "\n",
    "# Start the timer\n",
    "start_time = time()\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "classification = classification_report(y_test, y_pred)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time() - start_time\n",
    "# Convert elapsed time to hours, minutes, and seconds\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "# Display the elapsed time\n",
    "print(\"\\nStep 9: Evaluation DONE!\")\n",
    "print(f\"Elapsed time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# Step 10: Display the results\n",
    "# Start the timer\n",
    "start_time = time()\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "print(\"Classification Report:\")\n",
    "print(classification)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "absolute_end_time = time()\n",
    "elapsed_time = absolute_end_time - start_time\n",
    "# Convert elapsed time to hours, minutes, and seconds\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "# Display the elapsed time\n",
    "print(\"\\nStep 10: Display the results DONE!\")\n",
    "print(f\"Elapsed time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# Step 11: Dump y_pred, y_pred_proba using Pickle\n",
    "\n",
    "# Start the timer\n",
    "start_time = time()\n",
    "\n",
    "with open('data/y_pred_xgboost_200est_LR02_ngram13_maxdf9_pos_ner_counts_strat_feat_100k.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred, f)\n",
    "\n",
    "print(\"\\nStep 11: Dump y_pred using Pickle DONE!\")\n",
    "print(\"calculating y_pred proba\")\n",
    "# * Disable this step if needed\n",
    "y_pred_proba = classifier.predict_proba(X_test_vectorized)\n",
    "\n",
    "with open('data/y_pred_proba_xgboost_200est_LR02_ngram13_maxdf9_pos_ner_counts_strat_feat_100k.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred_proba, f)\n",
    "\n",
    "print(\"\\nStep 11: Dump y_pred_proba using Pickle DONE!\")\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "# TOTAL TIME CALCULATION:\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = absolute_end_time - absolute_start_time\n",
    "# Convert elapsed time to hours, minutes, and seconds\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "# Display the elapsed time\n",
    "print(f\"\\nTOTAL ELAPSED TIME: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
